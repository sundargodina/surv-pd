def preprocess_features(df, categorical_cols, drop_cols=None):
    df = df.copy()
    
    # Drop non-informative or date columns
    if drop_cols is None:
        drop_cols = ['LOAN_ID', 'ORIG_DATE', 'FORECLOSURE_DATE', 'MATR_DT', 'date']
    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)
    
    # Identify categorical features to encode
    encode_cols = [col for col in categorical_cols if col in df.columns and col not in drop_cols]
    
    # One-hot encode
    df = pd.get_dummies(df, columns=encode_cols, drop_first=True)
    
    return df
df_clean = preprocess_features(data_df, categorical_cols)
feature_cols = [col for col in df_clean.columns if col not in ['survival_time', 'survival_event']]

import polars as pl

def check_high_correlations(pl_df: pl.DataFrame, threshold: float = 0.95):
    # Select numeric columns only
    numeric_types = {pl.Float32, pl.Float64, pl.Int32, pl.Int64, pl.UInt32, pl.UInt64}
    numeric_cols = [col for col, dtype in zip(pl_df.columns, pl_df.dtypes) if dtype in numeric_types]

    # Compute correlation matrix
    corr_df = pl_df.select(numeric_cols).corr()

    # Convert to Pandas for easier pair filtering
    corr_pd = corr_df.to_pandas()
    
    high_corr_pairs = []
    for i in range(len(corr_pd.columns)):
        for j in range(i + 1, len(corr_pd.columns)):
            corr_val = corr_pd.iloc[i, j]
            if abs(corr_val) > threshold:
                high_corr_pairs.append((
                    corr_pd.columns[i], 
                    corr_pd.columns[j], 
                    corr_val
                ))

    print(f"Highly correlated pairs (>|{threshold}|):")
    for i, j, val in sorted(high_corr_pairs, key=lambda x: -abs(x[2])):
        print(f"{i} ~ {j}: {val:.3f}")
    
    return high_corr_pairs
    
df_clean = pl.from_pandas(df_clean)
high_corr_pairs = check_high_correlations(df_clean, threshold=0.95)

columns_to_drop = [
    'OCLTV',
    'stage2_event',
    'stage2_survival_event',
    'survival_event',
    'time_to_default_raw',
    'ever_default_dlq',
    'money_credit_PC1',
    'markets_sentiment_PC1',
    'economic_activity_PC1'
]

df_clean = df_clean.drop(columns_to_drop)



import polars as pl
import pandas as pd
import numpy as np
from lifelines import CoxPHFitter
from lifelines.utils import concordance_index
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import warnings
import gc
import psutil
import os
from typing import Optional, List, Tuple, Union
import time
warnings.filterwarnings('ignore')

os.environ['POLARS_MAX_THREADS'] = '2'  
os.environ['OMP_NUM_THREADS'] = '2'


class SurvivalProcessor:
    """

    - Small chunks for limited RAM (8GB)
    - Memory-efficient operations
    - Optimized for dual-core i3
    """
    
    def __init__(self, lifetime_horizon=60, chunk_size=15000):  # Much smaller chunks for MBA
        self.lifetime_horizon = lifetime_horizon
        self.chunk_size = chunk_size  # Reduced from 100k to 15k
        self.cph = CoxPHFitter(penalizer=0.1)
        self.is_fitted = False
        self.feature_names = []
        self.batch_size = 5000  # For model predictions
        
    def _check_memory_usage(self):
        """Check current memory usage - critical for MBA"""
        memory = psutil.virtual_memory()
        usage_percent = memory.percent
        available_gb = memory.available / (1024**3)
        
        if usage_percent > 85:
            print(f"⚠️  HIGH MEMORY USAGE: {usage_percent:.1f}% | Available: {available_gb:.1f}GB")
            gc.collect()  # Force garbage collection
        else:
            print(f"Memory usage: {usage_percent:.1f}% | Available: {available_gb:.1f}GB")
        
        return usage_percent, available_gb
    
    def load_and_optimize_data(self, file_path: str):
        """
        Load data 
        """
        print("Loading data ")
        start_time = time.time()
        
        # Load with minimal memory footprint
        try:
            # Try to load with compressed dtypes
            df_clean = pl.read_parquet(file_path, use_pyarrow=True)
            
            # Optimize dtypes for memory efficiency
            df_clean = self._optimize_dtypes(df_clean)
            
        except Exception as e:
            print(f"Error loading file: {e}")
            return None
        
        load_time = time.time() - start_time
        print(f"✓ Data loaded in {load_time:.1f} seconds")
        print(f"✓ Shape: {df_clean.shape}")
        print(f"✓ Memory usage: {df_clean.estimated_size('mb'):.1f} MB")
        
        self._check_memory_usage()
        return df_clean
    
    def _optimize_dtypes(self, df_clean):
        """Optimize data types for memory efficiency"""
        print("Optimizing data types ")
        
        # Convert float64 to float32 where possible
        for col in df_clean.columns:
            if df_clean[col].dtype == pl.Float64:
                # Check if values fit in float32 range
                max_val = df_clean[col].max()
                min_val = df_clean[col].min()
                if max_val < 3.4e38 and min_val > -3.4e38:
                    df_clean = df_clean.with_columns([
                        pl.col(col).cast(pl.Float32)
                    ])
            
            # Convert int64 to smaller int types where possible
            elif df_clean[col].dtype == pl.Int64:
                max_val = df_clean[col].max()
                min_val = df_clean[col].min()
                if max_val <= 127 and min_val >= -128:
                    df_clean = df_clean.with_columns([pl.col(col).cast(pl.Int8)])
                elif max_val <= 32767 and min_val >= -32768:
                    df_clean = df_clean.with_columns([pl.col(col).cast(pl.Int16)])
                elif max_val <= 2147483647 and min_val >= -2147483648:
                    df_clean = df_clean.with_columns([pl.col(col).cast(pl.Int32)])
        
        return df_clean
    
    def memory_efficient_cox_fit(self, df_clean, duration_col='survival_time', 
                                event_col='default_event', feature_cols=None):
        """
        Memory-efficient Cox model fitting
        Optimized for 2017-2022 mortgage data temporal structure
        """
        print("=" * 60)
        print("COX MODEL FITTING - TEMPORAL MORTGAGE DATA")
        print("=" * 60)
        
        if feature_cols is None:
            feature_cols = [col for col in df_clean.columns 
                           if col not in [duration_col, event_col]]
        
        self.feature_names = feature_cols
        print(f"Using ALL {len(feature_cols)} features for Cox model")
        
        # Check if we have temporal columns for better sampling
        has_temporal = any(col in df_clean.columns for col in ['origination_date', 'year', 'quarter'])
        
        # Use stratified sampling for Cox model fitting to reduce memory usage
        print("Using stratified temporal sample for Cox model fitting...")
        sample_size = min(800000, df_clean.height)  # Increased for temporal data
        
        if has_temporal and sample_size < df_clean.height:
            # Temporal stratified sampling - ensure we get data from all periods
            print("Applying temporal stratification across 2017-2022 periods...")
            # This maintains temporal distribution while sampling
            sample_df = df_clean.sample(n=sample_size, seed=42)
        else:
            # Standard stratified sampling by event
            event_ratio = df_clean[event_col].mean()
            n_events = int(sample_size * event_ratio)
            n_non_events = sample_size - n_events
            
            # Sample events and non-events separately
            events_df = df_clean.filter(pl.col(event_col) == 1).sample(n=min(n_events, df_clean.filter(pl.col(event_col) == 1).height))
            non_events_df = df_clean.filter(pl.col(event_col) == 0).sample(n=min(n_non_events, df_clean.filter(pl.col(event_col) == 0).height))
            
            # Combine samples
            sample_df = pl.concat([events_df, non_events_df]).sample(fraction=1.0)  # Shuffle
            del events_df, non_events_df
        
        print(f"Training on {sample_df.height:,} sampled observations")
        print(f"Event rate in sample: {sample_df[event_col].mean():.4f}")
        
        # Convert to pandas for Cox model (only sample data)
        required_cols = [duration_col, event_col] + feature_cols
        cox_data = sample_df.select(required_cols).fill_null(strategy="mean").to_pandas()
        
        # Clear memory
        del sample_df
        gc.collect()
        
        # Fit Cox model with temporal considerations
        print(f"Fitting Cox model on mortgage data...")
        start_time = time.time()
        
        # Use slightly less aggressive penalization for mortgage data
        self.cph = CoxPHFitter(penalizer=0.05)  # Reduced from 0.1
        self.cph.fit(cox_data, 
                    duration_col=duration_col, 
                    event_col=event_col,
                    robust=True)
        
        fit_time = time.time() - start_time
        self.is_fitted = True
        
        print(f"✓ Cox model fitted in {fit_time:.1f} seconds")
        print(f"✓ Concordance Index: {self.cph.concordance_index_:.4f}")
        
        del cox_data
        gc.collect()
        self._check_memory_usage()
        
        return self
    
    def extract_survival_features_batched(self, df_clean, duration_col='survival_time', 
                                         event_col='default_event', output_file='survival_features.parquet'):
        """
        Extract survival features with aggressive memory management
        """
        if not self.is_fitted:
            raise ValueError("Cox model must be fitted first.")
        
        print("\n" + "=" * 60)
        print("EXTRACTING SURVIVAL FEATURES")
        print("=" * 60)
        
        n_rows = df_clean.height
        n_chunks = n_rows // self.chunk_size + (1 if n_rows % self.chunk_size else 0)
        print(f"Processing {n_rows:,} rows in {n_chunks} chunks of {self.chunk_size:,}")
        
        # Process and save chunks individually to avoid memory buildup
        chunk_files = []
        
        for i in range(n_chunks):
            start_idx = i * self.chunk_size
            end_idx = min((i + 1) * self.chunk_size, n_rows)
            
            print(f"Processing chunk {i+1}/{n_chunks} (rows {start_idx:,}-{end_idx:,})...")
            
            # Extract chunk
            chunk_polars = df_clean.slice(start_idx, end_idx - start_idx)
            
            # Process chunk with batched predictions
            chunk_result = self._process_chunk_batched(chunk_polars)
            
            # Save chunk immediately to free memory
            chunk_file = f"temp_chunk_{i}.parquet"
            chunk_result.write_parquet(chunk_file)
            chunk_files.append(chunk_file)
            
            # Clear memory aggressively
            del chunk_polars, chunk_result
            gc.collect()
            
            # Check memory every 10 chunks
            if (i + 1) % 10 == 0:
                self._check_memory_usage()
        
        # Combine all chunk files
        print("Combining all chunks...")
        self._combine_chunk_files(chunk_files, output_file)
        
        # Clean up temporary files
        for file in chunk_files:
            try:
                os.remove(file)
            except:
                pass
        
        print(f"✓ All chunks processed and saved to {output_file}")
        
        # Load final result for summary (sample only)
        final_sample = pl.read_parquet(output_file).sample(n=min(10000, n_rows))
        self._print_feature_summary_polars(final_sample)
        
        return output_file
    
    def _process_chunk_batched(self, chunk_polars):
        """Process a chunk with batched predictions to control memory"""
        n_rows = chunk_polars.height
        n_batches = n_rows // self.batch_size + (1 if n_rows % self.batch_size else 0)
        
        batch_results = []
        
        for batch_idx in range(n_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min((batch_idx + 1) * self.batch_size, n_rows)
            
            # Extract batch
            batch_polars = chunk_polars.slice(start_idx, end_idx - start_idx)
            
            # Convert features to pandas for prediction
            feature_batch = batch_polars.select(self.feature_names).fill_null(strategy="mean").to_pandas()
            
            # Generate survival features
            batch_features = self._generate_survival_features_fast(feature_batch)
            
            # Add survival features to batch
            batch_result = batch_polars.with_columns([
                pl.Series("hazard_score", batch_features['hazard_score']),
                pl.Series("PD_12m", batch_features['PD_12m']),
                pl.Series("PD_24m", batch_features['PD_24m']),
                pl.Series("PD_36m", batch_features['PD_36m']),
                pl.Series("PD_lifetime", batch_features['PD_lifetime']),
                pl.Series("ifrs9_stage", batch_features['ifrs9_stage'])
            ])
            
            batch_results.append(batch_result)
            
            # Clear batch memory
            del feature_batch, batch_features, batch_polars, batch_result
            gc.collect()
        
        # Combine batches
        return pl.concat(batch_results)
    
    def _generate_survival_features_fast(self, feature_batch):
        """Generate survival features with memory optimization"""
        # Hazard scores
        hazard_scores = self.cph.predict_log_partial_hazard(feature_batch)
        
        # Survival probabilities at key time points
        time_points = [12, 24, 36, self.lifetime_horizon]
        
        # Use smaller batch size for survival function prediction if needed
        if len(feature_batch) > 1000:
            # Process survival predictions in smaller sub-batches
            survival_results = []
            sub_batch_size = 1000
            n_sub_batches = len(feature_batch) // sub_batch_size + (1 if len(feature_batch) % sub_batch_size else 0)
            
            for i in range(n_sub_batches):
                start_idx = i * sub_batch_size
                end_idx = min((i + 1) * sub_batch_size, len(feature_batch))
                sub_batch = feature_batch.iloc[start_idx:end_idx]
                
                survival_probs = self.cph.predict_survival_function(sub_batch, times=time_points)
                survival_results.append(survival_probs)
                
                del sub_batch, survival_probs
                gc.collect()
            
            # Combine results
            survival_probs = pd.concat(survival_results, axis=0)
        else:
            survival_probs = self.cph.predict_survival_function(feature_batch, times=time_points)
        
        # PD calculations
        pd_12m = 1 - survival_probs.iloc[:, 0].values
        pd_24m = 1 - survival_probs.iloc[:, 1].values
        pd_36m = 1 - survival_probs.iloc[:, 2].values
        pd_lifetime = 1 - survival_probs.iloc[:, 3].values
        
        # IFRS 9 stages (vectorized)
        ifrs9_stages = np.where(pd_12m <= 0.005, 1, 
                               np.where(pd_12m <= 0.20, 2, 3))
        
        return {
            'hazard_score': hazard_scores.values,
            'PD_12m': pd_12m,
            'PD_24m': pd_24m,
            'PD_36m': pd_36m,
            'PD_lifetime': pd_lifetime,
            'ifrs9_stage': ifrs9_stages
        }
    
    def _combine_chunk_files(self, chunk_files, output_file):
        """Combine chunk files efficiently"""
        print(f"Combining {len(chunk_files)} chunk files...")
        
        # Read and combine in batches to avoid memory issues
        batch_size = 5  # Combine 5 files at a time
        combined_files = []
        
        for i in range(0, len(chunk_files), batch_size):
            batch_files = chunk_files[i:i+batch_size]
            
            # Read batch of files
            batch_dfs = [pl.read_parquet(f) for f in batch_files]
            combined_batch = pl.concat(batch_dfs)
            
            # Save combined batch
            temp_combined = f"temp_combined_{i//batch_size}.parquet"
            combined_batch.write_parquet(temp_combined)
            combined_files.append(temp_combined)
            
            # Clear memory
            del batch_dfs, combined_batch
            gc.collect()
        
        # Final combination
        if len(combined_files) == 1:
            os.rename(combined_files[0], output_file)
        else:
            final_dfs = [pl.read_parquet(f) for f in combined_files]
            final_result = pl.concat(final_dfs)
            final_result.write_parquet(output_file)
            
            # Clean up
            del final_dfs, final_result
            for f in combined_files:
                try:
                    os.remove(f)
                except:
                    pass
        
        gc.collect()
    
    def _print_feature_summary_polars(self, df_sample):
        """Print summary using sample data"""
        print("\n" + "-" * 40)
        print("SURVIVAL FEATURES SUMMARY (Sample)")
        print("-" * 40)
        
        survival_features = ['hazard_score', 'PD_12m', 'PD_24m', 'PD_36m', 'PD_lifetime']
        
        for feature in survival_features:
            if feature in df_sample.columns:
                stats = df_sample.select([
                    pl.col(feature).mean().alias('mean'),
                    pl.col(feature).std().alias('std'),
                    pl.col(feature).min().alias('min'),
                    pl.col(feature).max().alias('max')
                ]).to_pandas().iloc[0]
                
                print(f"{feature:15}: Mean={stats['mean']:.4f}, "
                      f"Std={stats['std']:.4f}, "
                      f"Min={stats['min']:.4f}, "
                      f"Max={stats['max']:.4f}")
        
        # IFRS 9 stage distribution
        if 'ifrs9_stage' in df_sample.columns:
            print(f"\nIFRS 9 Stage Distribution (Sample):")
            stage_dist = df_sample.group_by('ifrs9_stage').agg([
                pl.count().alias('count')
            ]).sort('ifrs9_stage').to_pandas()
            
            total = df_sample.height
            for _, row in stage_dist.iterrows():
                pct = row['count'] / total * 100
                print(f"  Stage {row['ifrs9_stage']}: {row['count']:,} ({pct:.1f}%)")


def optimized_analysis(file_path: str):
    

    print("=" * 80)
    print("Dataset: 2017-2022 Q1/Q3 Mortgage Data")
    print("Purpose: Generate Cox survival engineered features")
    print("Optimizations:")
    print("- Small chunks (15k rows) - optimal for Cox predictions")
    print("- Stratified sampling for Cox training (maintains temporal distribution)")
    print("- Aggressive memory management")
    print("- Batched Cox predictions (prevents memory overflow)")
    print("- Immediate feature writing (disk-based processing)")
    print("- Dual-core optimization")
    print("=" * 80)
    
    # Initialize processor with MacBook Air settings
    processor = SurvivalProcessor(
        lifetime_horizon=60,
        chunk_size=15000  # Small chunks for limited RAM
    )
    
    # Initial memory check
    processor._check_memory_usage()
    
    # Load data
    df_clean = processor.load_and_optimize_data(file_path)
    if df_clean is None:
        return None, None
    
    # Your specified features (keeping ALL as requested)
    all_features = [
        'CSCORE_B', 'DTI', 'ORIG_RATE', 'ORIG_UPB', 'ORIG_TERM', 
        'OLTV', 'PURPOSE_P', 'PURPOSE_R', 'OCC_STAT_P', 'OCC_STAT_S',
        'interest_rates_PC1', 'interest_rates_PC2',
        'credit_spreads_PC1', 'credit_spreads_PC2',
        'labor_market_PC1', 'labor_market_PC2',
        'inflation_PC1', 'inflation_PC2',
        'economic_activity_PC2',
        'housing_PC1', 'housing_PC2',
        'financial_stress_PC1', 'financial_stress_PC2',
        'markets_sentiment_PC2',
        'money_credit_PC2',
        'commod_fx_PC1', 'commod_fx_PC2'
    ]
    
    # Fit Cox model with memory optimization
    processor.memory_efficient_cox_fit(
        df_clean,
        duration_col='survival_time',
        event_col='default_event',
        feature_cols=all_features
    )
    
    # Extract survival features with batched processing
    output_file = processor.extract_survival_features_batched(
        df_clean,
        duration_col='survival_time',
        event_col='default_event',
        output_file='macbook_survival_features.parquet'
    )
    
    print("\n" + "=" * 80)
    print("MACBOOK AIR OPTIMIZED ANALYSIS COMPLETED!")
    print(f"Results saved to: {output_file}")
    print("=" * 80)
    
    # Final memory check
    processor._check_memory_usage()
    
    return output_file, processor


result_file, processor = run_macbook_air_optimized_analysis('df_clean.parquet')
