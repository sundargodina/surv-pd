def preprocess_features(df, categorical_cols, drop_cols=None):
    df = df.copy()
    
    # Drop non-informative or date columns
    if drop_cols is None:
        drop_cols = ['LOAN_ID', 'ORIG_DATE', 'FORECLOSURE_DATE', 'MATR_DT', 'date']
    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)
    
    # Identify categorical features to encode
    encode_cols = [col for col in categorical_cols if col in df.columns and col not in drop_cols]
    
    # One-hot encode
    df = pd.get_dummies(df, columns=encode_cols, drop_first=True)
    
    return df
df_clean = preprocess_features(data_df, categorical_cols)
feature_cols = [col for col in df_clean.columns if col not in ['survival_time', 'survival_event']]

import polars as pl

def check_high_correlations(pl_df: pl.DataFrame, threshold: float = 0.95):
    # Select numeric columns only
    numeric_types = {pl.Float32, pl.Float64, pl.Int32, pl.Int64, pl.UInt32, pl.UInt64}
    numeric_cols = [col for col, dtype in zip(pl_df.columns, pl_df.dtypes) if dtype in numeric_types]

    # Compute correlation matrix
    corr_df = pl_df.select(numeric_cols).corr()

    # Convert to Pandas for easier pair filtering
    corr_pd = corr_df.to_pandas()
    
    high_corr_pairs = []
    for i in range(len(corr_pd.columns)):
        for j in range(i + 1, len(corr_pd.columns)):
            corr_val = corr_pd.iloc[i, j]
            if abs(corr_val) > threshold:
                high_corr_pairs.append((
                    corr_pd.columns[i], 
                    corr_pd.columns[j], 
                    corr_val
                ))

    print(f"Highly correlated pairs (>|{threshold}|):")
    for i, j, val in sorted(high_corr_pairs, key=lambda x: -abs(x[2])):
        print(f"{i} ~ {j}: {val:.3f}")
    
    return high_corr_pairs
    
df_clean = pl.from_pandas(df_clean)
high_corr_pairs = check_high_correlations(df_clean, threshold=0.95)

columns_to_drop = [
    'OCLTV',
    'stage2_event',
    'stage2_survival_event',
    'survival_event',
    'time_to_default_raw',
    'ever_default_dlq',
    'money_credit_PC1',
    'markets_sentiment_PC1',
    'economic_activity_PC1'
]

df_clean = df_clean.drop(columns_to_drop)


import polars as pl
import pandas as pd
import numpy as np
from lifelines import CoxPHFitter
import warnings
import gc
import psutil
import os
import time
from typing import List
warnings.filterwarnings('ignore')

# MacBook Air optimizations
os.environ['POLARS_MAX_THREADS'] = '2'
os.environ['OMP_NUM_THREADS'] = '2'

class FastCoxFeatureEngine:
    def __init__(self, lifetime_horizon=60):
        self.lifetime_horizon = lifetime_horizon
        self.cph = None
        self.is_fitted = False
        self.feature_names = []

    def fast_fit(self, df_polars, duration_col='survival_time', 
                 event_col='default_event', feature_cols=None, 
                 max_train_size=150000):

        print("\n FAST COX FITTING\n" + "=" * 50)

        if feature_cols is None:
            feature_cols = [col for col in df_polars.columns 
                            if col not in [duration_col, event_col]]

        self.feature_names = feature_cols
        print(f"Features: {len(feature_cols)}")

        n = df_polars.height
        if n > max_train_size:
            print(f"Stratified sampling {max_train_size:,} rows from {n:,}...")

            # Separate groups by event_col
            df_event_0 = df_polars.filter(pl.col(event_col) == 0)
            df_event_1 = df_polars.filter(pl.col(event_col) == 1)

            # Compute sample sizes proportionally
            size_0 = int(max_train_size * df_event_0.height / n)
            size_1 = max_train_size - size_0

            # Sample from each group
            sample_0 = df_event_0.sample(n=size_0, seed=42)
            sample_1 = df_event_1.sample(n=size_1, seed=42)

            # Concatenate stratified samples
            train_df = pl.concat([sample_0, sample_1])

        else:
            train_df = df_polars

        required_cols = [duration_col, event_col] + feature_cols
        cox_data = train_df.select(required_cols).fill_null(0).to_pandas()

        print("Fitting Cox model...")
        start = time.time()

        self.cph = CoxPHFitter(penalizer=0.01)
        self.cph.fit(cox_data, duration_col=duration_col, event_col=event_col)

        print(f"Fitted in {time.time() - start:.1f}s | C-index: {self.cph.concordance_index_:.3f}")
        self.is_fitted = True

        del cox_data, train_df, df_event_0, df_event_1, sample_0, sample_1
        gc.collect()
        return self

    def fast_predict_chunk(self, chunk_df):
        feature_data = chunk_df.select(self.feature_names).fill_null(0).to_pandas()
        hazard_scores = self.cph.predict_log_partial_hazard(feature_data).values

        time_points = [12, 24, 36, self.lifetime_horizon]
        surv_probs = self.cph.predict_survival_function(feature_data, times=time_points).T

        pd_12m, pd_24m, pd_36m, pd_lifetime = [
            1 - surv_probs[tp].values for tp in time_points
        ]
        ifrs9_stages = np.where(pd_12m <= 0.005, 1, np.where(pd_12m <= 0.20, 2, 3))

        return chunk_df.with_columns([
            pl.Series("hazard_score", hazard_scores),
            pl.Series("PD_12m", pd_12m),
            pl.Series("PD_24m", pd_24m),
            pl.Series("PD_36m", pd_36m),
            pl.Series("PD_lifetime", pd_lifetime),
            pl.Series("ifrs9_stage_cox", ifrs9_stages)
        ])



    def process_all_fast(self, df_polars, chunk_size=30000, output_file='cox_features.parquet'):
        if not self.is_fitted:
            raise ValueError("Model not fitted yet.")

        print(f"\n PROCESSING {df_polars.height:,} ROWS\n" + "=" * 50)
        chunks = [df_polars.slice(i, min(chunk_size, df_polars.height - i)) for i in range(0, df_polars.height, chunk_size)]

        start_time = time.time()
        all_chunks = []

        for i, chunk in enumerate(chunks):
            if i % 10 == 0:
                elapsed = time.time() - start_time
                print(f"Chunk {i+1}/{len(chunks)} | Elapsed: {elapsed/60:.1f}min")

            result = self.fast_predict_chunk(chunk)
            all_chunks.append(result)
            del chunk, result
            gc.collect()

        final_result = pl.concat(all_chunks)
        final_result.write_parquet(output_file)

        print(f"Done in {(time.time() - start_time)/60:.1f} minutes | Output: {output_file}")
        return output_file

def run_fast_cox_engineering(file_path: str, 
                              duration_col='survival_time',
                              event_col='default_event',
                              feature_cols=None):
    print("âš¡ ULTRA-FAST COX FEATURE ENGINEERING\nðŸ–¥ï¸ MacBook Air i3 2020 Optimized\n" + "=" * 60)
    print(f"RAM Usage: {psutil.virtual_memory().percent:.1f}% | Available: {psutil.virtual_memory().available/1024**3:.1f} GB\n")

    print("ðŸ“‚ Loading data...")
    try:
        df = pl.read_parquet(file_path)
        print(f"{df.height:,} rows loaded | Memory: {df.estimated_size('mb'):.0f}MB")
    except Exception as e:
        print(f" Load failed: {e}")
        return None

    if feature_cols is None:
        feature_cols = [
            'CSCORE_B', 'DTI', 'ORIG_RATE', 'ORIG_UPB', 'ORIG_TERM', 'OLTV',
            'PURPOSE_P', 'PURPOSE_R', 'OCC_STAT_P', 'OCC_STAT_S',
            'interest_rates_PC1', 'interest_rates_PC2',
            'credit_spreads_PC1', 'credit_spreads_PC2',
            'labor_market_PC1', 'labor_market_PC2',
            'inflation_PC1', 'inflation_PC2',
            'economic_activity_PC2', 'housing_PC1', 'housing_PC2',
            'financial_stress_PC1', 'financial_stress_PC2',
            'markets_sentiment_PC2', 'money_credit_PC2',
            'commod_fx_PC1', 'commod_fx_PC2']

    engine = FastCoxFeatureEngine(lifetime_horizon=60)
    engine.fast_fit(df, duration_col, event_col, feature_cols)
    output_file = engine.process_all_fast(df, chunk_size=30000)

    print("\n Summary:")
    sample = pl.read_parquet(output_file).sample(n=1000)
    for col in ['PD_12m', 'PD_24m', 'PD_lifetime']:
        if col in sample.columns:
            print(f"{col}: {sample[col].mean():.4f}")

    if 'ifrs9_stage' in sample.columns:
        print("\nIFRS 9 Stage Distribution:")
        stage_counts = sample['ifrs9_stage'].value_counts().sort('ifrs9_stage')
        for stage, count in stage_counts.iter_rows():
            print(f"Stage {stage}: {count/sample.height*100:.1f}%")

    print(f"\nSuccess! Output saved to {output_file}")
    return output_file
    
run_fast_cox_engineering("df_clean.parquet")
